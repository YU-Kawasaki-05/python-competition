{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# NFL Draft Prediction - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
        "\n",
        "## æ¦‚è¦\n",
        "EDAã§å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹ã‚’åŸºã«ã€äºˆæ¸¬æ€§èƒ½å‘ä¸Šã®ãŸã‚ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚\n",
        "\n",
        "## ç›®æ¨™\n",
        "- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ã‚³ã‚¢0.80792ã‚’ä¸Šå›ã‚‹\n",
        "- ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’æ´»ç”¨ã—ãŸæœ‰åŠ¹ãªç‰¹å¾´é‡ä½œæˆ\n",
        "- æ¬ æå€¤å‡¦ç†ã®æœ€é©åŒ–\n",
        "- ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ã®ç‰¹å¾´é‡ä½œæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "\n",
        "# è¨­å®š\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "print(\"ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿å®Œäº†\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
        "train = pd.read_csv('data/train.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "sample_submission = pd.read_csv('data/sample_submission_6_20ä¿®æ­£.csv')\n",
        "\n",
        "print(f\"Train: {train.shape}, Test: {test.shape}\")\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒï¼‰\n",
        "train_fe = train.copy()\n",
        "test_fe = test.copy()\n",
        "\n",
        "# å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãŸã‚ï¼‰\n",
        "all_data = pd.concat([train_fe, test_fe], ignore_index=True)\n",
        "print(f\"çµåˆãƒ‡ãƒ¼ã‚¿: {all_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. åŸºæœ¬ç‰¹å¾´é‡ã®ä½œæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŸºæœ¬çš„ãªçµ„ã¿åˆã‚ã›ç‰¹å¾´é‡ã®ä½œæˆ\n",
        "def create_basic_features(df):\n",
        "    \\\"\\\"\\\"åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚’ä½œæˆ\\\"\\\"\\\"\n",
        "    df_new = df.copy()\n",
        "    \n",
        "    # 1. BMIï¼ˆBody Mass Indexï¼‰\n",
        "    df_new['BMI'] = df_new['Weight'] / (df_new['Height'] ** 2)\n",
        "    \n",
        "    # 2. ãƒ‘ãƒ¯ãƒ¼ãƒ»ã‚¦ã‚§ã‚¤ãƒˆãƒ»ãƒ¬ã‚·ã‚ª\n",
        "    df_new['Power_Weight_Ratio'] = df_new['Bench_Press_Reps'] / df_new['Weight']\n",
        "    \n",
        "    # 3. ã‚¹ãƒ”ãƒ¼ãƒ‰ãƒ»ãƒ‘ãƒ¯ãƒ¼ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå‚ç›´è·³ã³ / 40ãƒ¤ãƒ¼ãƒ‰èµ°ï¼‰\n",
        "    df_new['Speed_Power_Index'] = df_new['Vertical_Jump'] / df_new['Sprint_40yd']\n",
        "    \n",
        "    # 4. ã‚¢ã‚¸ãƒªãƒ†ã‚£ãƒ»ã‚¹ã‚³ã‚¢ï¼ˆ3cone + shuttleï¼‰\n",
        "    df_new['Agility_Score'] = df_new['Agility_3cone'] + df_new['Shuttle']\n",
        "    \n",
        "    # 5. çˆ†ç™ºåŠ›æŒ‡æ¨™ï¼ˆç«‹ã¡å¹…è·³ã³ * å‚ç›´è·³ã³ï¼‰\n",
        "    df_new['Explosiveness'] = df_new['Broad_Jump'] * df_new['Vertical_Jump']\n",
        "    \n",
        "    # 6. ã‚¹ãƒ”ãƒ¼ãƒ‰Ã—çˆ†ç™ºåŠ›\n",
        "    df_new['Speed_Explosiveness'] = df_new['Explosiveness'] / df_new['Sprint_40yd']\n",
        "    \n",
        "    # 7. ä½“æ ¼æŒ‡æ¨™ï¼ˆèº«é•·Ã—ä½“é‡ï¼‰\n",
        "    df_new['Size_Index'] = df_new['Height'] * df_new['Weight']\n",
        "    \n",
        "    # 8. ãƒ™ãƒ³ãƒãƒ—ãƒ¬ã‚¹åŠ¹ç‡ï¼ˆå›æ•°/ä½“é‡Ã—èº«é•·ï¼‰\n",
        "    df_new['Bench_Efficiency'] = df_new['Bench_Press_Reps'] / df_new['Size_Index']\n",
        "    \n",
        "    # 9. å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆã‚«ãƒ†ã‚´ãƒªåŒ–ï¼‰\n",
        "    df_new['Age_Group'] = pd.cut(df_new['Age'], bins=[0, 21, 23, 30], \n",
        "                                labels=['Young', 'Standard', 'Veteran'], \n",
        "                                include_lowest=True)\n",
        "    \n",
        "    print(\\\"åŸºæœ¬ç‰¹å¾´é‡ä½œæˆå®Œäº†\\\")\\n    return df_new\n",
        "\n",
        "# åŸºæœ¬ç‰¹å¾´é‡ã®ä½œæˆ\n",
        "all_data_fe = create_basic_features(all_data)\n",
        "\n",
        "# æ–°ç‰¹å¾´é‡ã®ç¢ºèª\n",
        "new_features = ['BMI', 'Power_Weight_Ratio', 'Speed_Power_Index', 'Agility_Score', \n",
        "                'Explosiveness', 'Speed_Explosiveness', 'Size_Index', 'Bench_Efficiency']\n",
        "\n",
        "print(f\\\"\\\\n=== æ–°ç‰¹å¾´é‡ã®çµ±è¨ˆ ===\\\")\n",
        "print(all_data_fe[new_features].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ç‰¹å¾´é‡ã®ä½œæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ã®ç‰¹å¾´é‡ä½œæˆ\n",
        "def create_position_features(df):\\n    \\\"\\\"\\\"ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ¥ã®ç‰¹å¾´é‡ã‚’ä½œæˆ\\\"\\\"\\\"\\n    df_new = df.copy()\\n    \\n    # ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã®æ­£è¦åŒ–ç‰¹å¾´é‡\\n    numerical_cols = ['Height', 'Weight', 'Sprint_40yd', 'Vertical_Jump', \\n                     'Bench_Press_Reps', 'Broad_Jump', 'Agility_3cone', 'Shuttle']\\n    \\n    for col in numerical_cols:\\n        if col in df_new.columns:\\n            # ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã®å¹³å‡å€¤ã§æ­£è¦åŒ–\\n            position_means = df_new.groupby('Position_Type')[col].transform('mean')\\n            df_new[f'{col}_vs_Position_Mean'] = df_new[col] / position_means\\n            \\n            # ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã®æ¨™æº–åå·®ã§æ­£è¦åŒ–\\n            position_stds = df_new.groupby('Position_Type')[col].transform('std')\\n            df_new[f'{col}_Position_Zscore'] = (df_new[col] - position_means) / position_stds\\n    \\n    # ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ã®çµ„ã¿åˆã‚ã›ç‰¹å¾´é‡\\n    # 1. ã‚ªãƒ•ã‚§ãƒ³ã‚·ãƒ–ãƒ©ã‚¤ãƒ³ç”¨ç‰¹å¾´é‡\\n    offensive_line_mask = df_new['Position_Type'] == 'offensive_lineman'\\n    df_new['OL_Strength_Index'] = 0\\n    df_new.loc[offensive_line_mask, 'OL_Strength_Index'] = (\\n        df_new.loc[offensive_line_mask, 'Bench_Press_Reps'] * \\n        df_new.loc[offensive_line_mask, 'Weight'] / 100\\n    )\\n    \\n    # 2. ã‚¹ã‚­ãƒ«ãƒã‚¸ã‚·ãƒ§ãƒ³ç”¨ç‰¹å¾´é‡\\n    skill_positions = ['backs_receivers']\\n    skill_mask = df_new['Position_Type'].isin(skill_positions)\\n    df_new['Skill_Speed_Index'] = 0\\n    df_new.loc[skill_mask, 'Skill_Speed_Index'] = (\\n        df_new.loc[skill_mask, 'Vertical_Jump'] * \\n        df_new.loc[skill_mask, 'Broad_Jump'] / \\n        df_new.loc[skill_mask, 'Sprint_40yd']\\n    )\\n    \\n    # 3. ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹ç”¨ç‰¹å¾´é‡\\n    defense_positions = ['defensive_lineman', 'line_backer', 'defensive_back']\\n    defense_mask = df_new['Position_Type'].isin(defense_positions)\\n    df_new['Defense_Athleticism'] = 0\\n    df_new.loc[defense_mask, 'Defense_Athleticism'] = (\\n        (df_new.loc[defense_mask, 'Vertical_Jump'] + \\n         df_new.loc[defense_mask, 'Broad_Jump']) / \\n        (df_new.loc[defense_mask, 'Sprint_40yd'] + \\n         df_new.loc[defense_mask, 'Agility_3cone'])\\n    )\\n    \\n    print(\\\"ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ç‰¹å¾´é‡ä½œæˆå®Œäº†\\\")\\n    return df_new\\n\\n# ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ç‰¹å¾´é‡ã®ä½œæˆ\\nall_data_fe = create_position_features(all_data_fe)\\n\\n# ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ç‰¹å¾´é‡ã®ç¢ºèª\\nposition_features = ['OL_Strength_Index', 'Skill_Speed_Index', 'Defense_Athleticism']\\nprint(f\\\"\\\\n=== ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ç‰¹å¾´é‡ã®çµ±è¨ˆ ===\\\")\\nprint(all_data_fe[position_features].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. å­¦æ ¡ãƒ—ãƒ¬ã‚¹ãƒ†ãƒ¼ã‚¸ç‰¹å¾´é‡ã®ä½œæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å­¦æ ¡ãƒ—ãƒ¬ã‚¹ãƒ†ãƒ¼ã‚¸ç‰¹å¾´é‡ã®ä½œæˆ\\ndef create_school_features(df):\\n    \\\"\\\"\\\"å­¦æ ¡ã«é–¢ã™ã‚‹ç‰¹å¾´é‡ã‚’ä½œæˆ\\\"\\\"\\\"\\n    df_new = df.copy()\\n    \\n    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ã£ã¦å­¦æ ¡çµ±è¨ˆã‚’è¨ˆç®—ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ãï¼‰\\n    train_data = df_new[df_new['Drafted'].notna()].copy()\\n    \\n    # å­¦æ ¡åˆ¥ã®æŒ‡åç‡ã¨é¸æ‰‹æ•°\\n    school_stats = train_data.groupby('School').agg({\\n        'Drafted': ['count', 'mean']\\n    })\\n    school_stats.columns = ['School_Player_Count', 'School_Draft_Rate']\\n    \\n    # 5äººæœªæº€ã®å­¦æ ¡ã¯çµ±è¨ˆãŒä¸å®‰å®šãªã®ã§å¹³å‡å€¤ã§è£œå®Œ\\n    overall_draft_rate = train_data['Drafted'].mean()\\n    school_stats.loc[school_stats['School_Player_Count'] < 5, 'School_Draft_Rate'] = overall_draft_rate\\n    \\n    # å­¦æ ¡ãƒ—ãƒ¬ã‚¹ãƒ†ãƒ¼ã‚¸ã‚«ãƒ†ã‚´ãƒª\\n    def categorize_school_prestige(draft_rate, player_count):\\n        if pd.isna(draft_rate):\\n            return 'Unknown'\\n        elif draft_rate >= 0.8:\\n            return 'Elite'\\n        elif draft_rate >= 0.6:\\n            return 'High'\\n        elif draft_rate >= 0.4:\\n            return 'Medium'\\n        else:\\n            return 'Low'\\n    \\n    school_stats['School_Prestige'] = school_stats.apply(\\n        lambda x: categorize_school_prestige(x['School_Draft_Rate'], x['School_Player_Count']), axis=1\\n    )\\n    \\n    # å…¨ãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸\\n    df_new = df_new.merge(school_stats, left_on='School', right_index=True, how='left')\\n    \\n    # æ–°ã—ã„å­¦æ ¡ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã¿ã«å­˜åœ¨ï¼‰ã¯å¹³å‡å€¤ã§è£œå®Œ\\n    df_new['School_Draft_Rate'].fillna(overall_draft_rate, inplace=True)\\n    df_new['School_Player_Count'].fillna(1, inplace=True)\\n    df_new['School_Prestige'].fillna('Unknown', inplace=True)\\n    \\n    print(\\\"å­¦æ ¡ãƒ—ãƒ¬ã‚¹ãƒ†ãƒ¼ã‚¸ç‰¹å¾´é‡ä½œæˆå®Œäº†\\\")\\n    return df_new\\n\\n# å­¦æ ¡ç‰¹å¾´é‡ã®ä½œæˆ\\nall_data_fe = create_school_features(all_data_fe)\\n\\n# å­¦æ ¡ãƒ—ãƒ¬ã‚¹ãƒ†ãƒ¼ã‚¸ã®åˆ†å¸ƒç¢ºèª\\nprint(f\\\"\\\\n=== å­¦æ ¡ãƒ—ãƒ¬ã‚¹ãƒ†ãƒ¼ã‚¸åˆ†å¸ƒ ===\\\")\\nprint(all_data_fe['School_Prestige'].value_counts())\\nprint(f\\\"\\\\n=== å­¦æ ¡æŒ‡åç‡çµ±è¨ˆ ===\\\")\\nprint(all_data_fe['School_Draft_Rate'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. æ¬ æå€¤å‡¦ç†ã®æœ€é©åŒ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ”¹è‰¯ã•ã‚ŒãŸæ¬ æå€¤å‡¦ç†\\ndef advanced_missing_value_handling(df):\\n    \\\"\\\"\\\"ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ¥ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®æ¬ æå€¤å‡¦ç†\\\"\\\"\\\"\\n    df_new = df.copy()\\n    \\n    # èº«ä½“èƒ½åŠ›ãƒ†ã‚¹ãƒˆç³»ã®åˆ—\\n    performance_cols = ['Sprint_40yd', 'Vertical_Jump', 'Bench_Press_Reps', \\n                       'Broad_Jump', 'Agility_3cone', 'Shuttle']\\n    \\n    # 1. æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ•ãƒ©ã‚°ã®ä½œæˆ\\n    for col in performance_cols:\\n        df_new[f'{col}_is_missing'] = df_new[col].isnull().astype(int)\\n    \\n    # 2. æ¬ æå€¤ã®ç·æ•°\\n    df_new['Total_Missing_Tests'] = df_new[[f'{col}_is_missing' for col in performance_cols]].sum(axis=1)\\n    \\n    # 3. ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ¥ã®ä¸­å¤®å€¤ã§è£œå®Œ\\n    for col in performance_cols:\\n        if col in df_new.columns:\\n            # ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã®ä¸­å¤®å€¤\\n            position_medians = df_new.groupby('Position_Type')[col].median()\\n            \\n            for position in position_medians.index:\\n                mask = (df_new['Position_Type'] == position) & (df_new[col].isnull())\\n                df_new.loc[mask, col] = position_medians[position]\\n            \\n            # æ®‹ã£ãŸæ¬ æå€¤ã¯å…¨ä½“ã®ä¸­å¤®å€¤ã§è£œå®Œ\\n            overall_median = df_new[col].median()\\n            df_new[col].fillna(overall_median, inplace=True)\\n    \\n    # 4. Age ã®è£œå®Œ\\n    if 'Age' in df_new.columns:\\n        # ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ¥ã®ä¸­å¤®å€¤ã§è£œå®Œ\\n        age_medians = df_new.groupby('Position_Type')['Age'].median()\\n        for position in age_medians.index:\\n            mask = (df_new['Position_Type'] == position) & (df_new['Age'].isnull())\\n            df_new.loc[mask, 'Age'] = age_medians[position]\\n        \\n        # æ®‹ã£ãŸæ¬ æå€¤ã¯å…¨ä½“ã®ä¸­å¤®å€¤ã§è£œå®Œ\\n        df_new['Age'].fillna(df_new['Age'].median(), inplace=True)\\n    \\n    print(\\\"æ”¹è‰¯ã•ã‚ŒãŸæ¬ æå€¤å‡¦ç†å®Œäº†\\\")\\n    return df_new\\n\\n# æ¬ æå€¤å‡¦ç†ã®å®Ÿè¡Œ\\nall_data_fe = advanced_missing_value_handling(all_data_fe)\\n\\n# æ¬ æå€¤å‡¦ç†å¾Œã®ç¢ºèª\\nprint(f\\\"\\\\n=== æ¬ æå€¤å‡¦ç†å¾Œã®çŠ¶æ³ ===\\\")\\nmissing_after = all_data_fe.isnull().sum()\\nprint(missing_after[missing_after > 0])\\n\\nif len(missing_after[missing_after > 0]) == 0:\\n    print(\\\"ã™ã¹ã¦ã®æ¬ æå€¤ãŒå‡¦ç†ã•ã‚Œã¾ã—ãŸï¼\\\")\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7. ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\\ndef encode_categorical_features(df):\\n    \\\"\\\"\\\"ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®åŠ¹æœçš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\\\"\\\"\\\"\\n    df_new = df.copy()\\n    \\n    # Label Encodingã™ã‚‹åˆ—\\n    label_encode_cols = ['School', 'Player_Type', 'Position_Type', 'Position', \\n                        'Age_Group', 'School_Prestige']\\n    \\n    encoders = {}\\n    for col in label_encode_cols:\\n        if col in df_new.columns:\\n            le = LabelEncoder()\\n            df_new[col] = le.fit_transform(df_new[col].astype(str))\\n            encoders[col] = le\\n    \\n    # å¹´åº¦ã®å‡¦ç†ï¼ˆæ—¢ã«æ•°å€¤ãªã®ã§ãã®ã¾ã¾ï¼‰\\n    # å¹´åº¦ã®å‘¨æœŸæ€§ã‚’è€ƒæ…®ã—ãŸç‰¹å¾´é‡\\n    if 'Year' in df_new.columns:\\n        df_new['Year_sin'] = np.sin(2 * np.pi * (df_new['Year'] - df_new['Year'].min()) / \\n                                   (df_new['Year'].max() - df_new['Year'].min()))\\n        df_new['Year_cos'] = np.cos(2 * np.pi * (df_new['Year'] - df_new['Year'].min()) / \\n                                   (df_new['Year'].max() - df_new['Year'].min()))\\n    \\n    print(\\\"ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†\\\")\\n    return df_new, encoders\\n\\n# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å®Ÿè¡Œ\\nall_data_fe, label_encoders = encode_categorical_features(all_data_fe)\\n\\n# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ç¢ºèª\\nprint(f\\\"\\\\n=== ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— ===\\\")\\nprint(all_data_fe.dtypes.value_counts())\\nprint(f\\\"\\\\n=== æ•°å€¤ä»¥å¤–ã®åˆ—ï¼ˆæ®‹ã£ã¦ã„ã‚‹å ´åˆï¼‰ ===\\\")\\nnon_numeric = all_data_fe.select_dtypes(exclude=[np.number]).columns\\nprint(list(non_numeric))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8. æœ€çµ‚çš„ãªç‰¹å¾´é‡é¸æŠã¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æœ€çµ‚çš„ãªç‰¹å¾´é‡é¸æŠã¨ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\\ndef prepare_final_dataset(df, train_len):\\n    \\\"\\\"\\\"æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\\\"\\\"\\\"\\n    # ä½¿ç”¨ã—ãªã„åˆ—ã‚’é™¤å¤–\\n    exclude_cols = ['Id', 'Drafted']\\n    feature_cols = [col for col in df.columns if col not in exclude_cols]\\n    \\n    # æ•°å€¤å‹ä»¥å¤–ã®åˆ—ãŒã‚ã‚Œã°ç¢ºèª\\n    non_numeric_cols = df[feature_cols].select_dtypes(exclude=[np.number]).columns\\n    if len(non_numeric_cols) > 0:\\n        print(f\\\"è­¦å‘Š: æ•°å€¤å‹ä»¥å¤–ã®åˆ—ãŒã‚ã‚Šã¾ã™: {list(non_numeric_cols)}\\\")\\n        # å¼·åˆ¶çš„ã«æ•°å€¤å‹ã«å¤‰æ›\\n        for col in non_numeric_cols:\\n            df[col] = pd.to_numeric(df[col], errors='coerce')\\n    \\n    # ç„¡é™å¤§ã‚„æ¥µç«¯ã«å¤§ããªå€¤ã®å‡¦ç†\\n    for col in feature_cols:\\n        if col in df.columns:\\n            # ç„¡é™å¤§ã‚’ NaN ã«å¤‰æ›\\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\\n            \\n            # æ®‹ã£ãŸ NaN ã‚’ä¸­å¤®å€¤ã§è£œå®Œ\\n            if df[col].isnull().sum() > 0:\\n                df[col].fillna(df[col].median(), inplace=True)\\n    \\n    # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²\\n    train_processed = df[:train_len].copy()\\n    test_processed = df[train_len:].copy()\\n    \\n    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢\\n    X_train = train_processed[feature_cols]\\n    y_train = train_processed['Drafted']\\n    X_test = test_processed[feature_cols]\\n    \\n    print(f\\\"ç‰¹å¾´é‡æ•°: {len(feature_cols)}\\\")\\n    print(f\\\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X_train.shape}\\\")\\n    print(f\\\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X_test.shape}\\\")\\n    print(f\\\"æ¬ æå€¤ç¢ºèª - è¨“ç·´: {X_train.isnull().sum().sum()}, ãƒ†ã‚¹ãƒˆ: {X_test.isnull().sum().sum()}\\\")\\n    \\n    return X_train, y_train, X_test, feature_cols\\n\\n# æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\\nX_train_fe, y_train_fe, X_test_fe, feature_cols_fe = prepare_final_dataset(all_data_fe, len(train))\\n\\n# ä½œæˆã•ã‚ŒãŸç‰¹å¾´é‡ã®ä¸€è¦§\\nprint(f\\\"\\\\n=== ä½œæˆã•ã‚ŒãŸç‰¹å¾´é‡ä¸€è¦§ï¼ˆç·æ•°: {len(feature_cols_fe)}ï¼‰ ===\\\")\\nfor i, col in enumerate(feature_cols_fe, 1):\\n    print(f\\\"{i:2d}. {col}\\\")\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 9. æ”¹è‰¯ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ãƒ»è©•ä¾¡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ”¹è‰¯ã•ã‚ŒãŸRandomForestãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡\n",
        "def evaluate_improved_model(X_train, y_train, n_splits=5, random_state=42):\n",
        "    \\\"\\\"\\\"æ”¹è‰¯ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®äº¤å·®æ¤œè¨¼è©•ä¾¡\\\"\\\"\\\"\\n    \\n    # Stratified K-Foldè¨­å®š\\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\\n    \\n    # ã‚¹ã‚³ã‚¢æ ¼ç´ç”¨\\n    cv_scores = []\\n    feature_importance_sum = np.zeros(X_train.shape[1])\\n    \\n    print(\\\"=== 5-Foldäº¤å·®æ¤œè¨¼å®Ÿè¡Œä¸­ ===\\\")\\n    \\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\\n        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²\\n        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\\n        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\\n        \\n        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ï¼‰\\n        rf_model = RandomForestClassifier(\\n            n_estimators=200,\\n            max_depth=10,\\n            min_samples_split=10,\\n            min_samples_leaf=5,\\n            max_features='sqrt',\\n            class_weight='balanced',\\n            random_state=random_state + fold,\\n            n_jobs=-1\\n        )\\n        \\n        rf_model.fit(X_fold_train, y_fold_train)\\n        \\n        # äºˆæ¸¬ãƒ»è©•ä¾¡\\n        y_pred_proba = rf_model.predict_proba(X_fold_val)[:, 1]\\n        fold_score = roc_auc_score(y_fold_val, y_pred_proba)\\n        cv_scores.append(fold_score)\\n        \\n        # ç‰¹å¾´é‡é‡è¦åº¦ã®ç´¯ç©\\n        feature_importance_sum += rf_model.feature_importances_\\n        \\n        print(f\\\"Fold {fold}: ROC AUC = {fold_score:.5f}\\\")\\n    \\n    # çµæœã®é›†è¨ˆ\\n    mean_score = np.mean(cv_scores)\\n    std_score = np.std(cv_scores)\\n    feature_importance_avg = feature_importance_sum / n_splits\\n    \\n    print(f\\\"\\\\n=== æœ€çµ‚çµæœ ===\\\")\\n    print(f\\\"å¹³å‡ROC AUC: {mean_score:.5f} Â± {std_score:.5f}\\\")\\n    print(f\\\"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ: {mean_score:.5f} vs 0.80792\\\")\\n    \\n    if mean_score > 0.80792:\\n        improvement = ((mean_score - 0.80792) / 0.80792) * 100\\n        print(f\\\"ğŸ‰ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ”¹å–„: +{improvement:.2f}%\\\")\\n    else:\\n        decline = ((0.80792 - mean_score) / 0.80792) * 100\\n        print(f\\\"âš ï¸  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä¸‹å›ã‚Š: -{decline:.2f}%\\\")\\n    \\n    return cv_scores, feature_importance_avg\\n\\n# æ”¹è‰¯ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡å®Ÿè¡Œ\\ncv_scores_fe, feature_importance_fe = evaluate_improved_model(X_train_fe, y_train_fe)\\n\\n# ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–\\nfeature_importance_df = pd.DataFrame({\\n    'feature': feature_cols_fe,\\n    'importance': feature_importance_fe\\n}).sort_values('importance', ascending=False)\\n\\nprint(f\\\"\\\\n=== ç‰¹å¾´é‡é‡è¦åº¦TOP20 ===\\\")\\nprint(feature_importance_df.head(20))\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–\n",
        "plt.figure(figsize=(12, 10))\n",
        "top_20_features = feature_importance_df.head(20)\n",
        "sns.barplot(data=top_20_features, y='feature', x='importance', palette='viridis')\n",
        "plt.title('ç‰¹å¾´é‡é‡è¦åº¦ TOP20ï¼ˆRandomForestï¼‰')\n",
        "plt.xlabel('é‡è¦åº¦')\n",
        "plt.ylabel('ç‰¹å¾´é‡')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# æ–°è¦ä½œæˆç‰¹å¾´é‡ã®è©•ä¾¡\n",
        "engineered_features = ['BMI', 'Power_Weight_Ratio', 'Speed_Power_Index', 'Agility_Score', \n",
        "                      'Explosiveness', 'Speed_Explosiveness', 'Size_Index', 'Bench_Efficiency',\n",
        "                      'OL_Strength_Index', 'Skill_Speed_Index', 'Defense_Athleticism',\n",
        "                      'School_Draft_Rate', 'Total_Missing_Tests']\n",
        "\n",
        "engineered_importance = feature_importance_df[feature_importance_df['feature'].isin(engineered_features)]\n",
        "print(f\"\\n=== æ–°è¦ä½œæˆç‰¹å¾´é‡ã®é‡è¦åº¦ ===\")\n",
        "print(engineered_importance.sort_values('importance', ascending=False))\n",
        "\n",
        "# æ–°è¦ç‰¹å¾´é‡ã®ç·åˆè²¢çŒ®åº¦\n",
        "total_engineered_importance = engineered_importance['importance'].sum()\n",
        "total_importance = feature_importance_df['importance'].sum()\n",
        "engineered_contribution = (total_engineered_importance / total_importance) * 100\n",
        "print(f\"\\næ–°è¦ç‰¹å¾´é‡ã®ç·åˆè²¢çŒ®åº¦: {engineered_contribution:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 10. æœ€çµ‚äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿äºˆæ¸¬\n",
        "def create_final_submission(X_train, y_train, X_test, test_ids, random_state=42):\n",
        "    \"\"\"æœ€çµ‚çš„ãªæå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\"\"\"\n",
        "    \n",
        "    print(\"=== æœ€çµ‚ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­ ===\")\n",
        "    \n",
        "    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šï¼ˆå…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ï¼‰\n",
        "    final_model = RandomForestClassifier(\n",
        "        n_estimators=300,  # å°‘ã—å¢—ã‚„ã—ã¦ã¿ã‚‹\n",
        "        max_depth=12,\n",
        "        min_samples_split=8,\n",
        "        min_samples_leaf=4,\n",
        "        max_features='sqrt',\n",
        "        class_weight='balanced',\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    # å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’\n",
        "    final_model.fit(X_train, y_train)\n",
        "    \n",
        "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬\n",
        "    test_predictions = final_model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
        "    submission = pd.DataFrame({\n",
        "        'Id': test_ids,\n",
        "        'Drafted': test_predictions\n",
        "    })\n",
        "    \n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\n",
        "    submission_filename = 'improved_submission_with_feature_engineering.csv'\n",
        "    submission.to_csv(submission_filename, index=False)\n",
        "    \n",
        "    print(f\"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {submission_filename}\")\n",
        "    print(f\"äºˆæ¸¬å€¤ã®çµ±è¨ˆ:\")\n",
        "    print(f\"  å¹³å‡: {test_predictions.mean():.4f}\")\n",
        "    print(f\"  ä¸­å¤®å€¤: {np.median(test_predictions):.4f}\")\n",
        "    print(f\"  æœ€å°å€¤: {test_predictions.min():.4f}\")\n",
        "    print(f\"  æœ€å¤§å€¤: {test_predictions.max():.4f}\")\n",
        "    print(f\"  æ¨™æº–åå·®: {test_predictions.std():.4f}\")\n",
        "    \n",
        "    return submission, final_model\n",
        "\n",
        "# æœ€çµ‚æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
        "test_ids = test['Id'].values\n",
        "final_submission, final_model = create_final_submission(X_train_fe, y_train_fe, X_test_fe, test_ids)\n",
        "\n",
        "# äºˆæ¸¬åˆ†å¸ƒã®å¯è¦–åŒ–\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(final_submission['Drafted'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.title('ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿äºˆæ¸¬å€¤ã®åˆ†å¸ƒ')\n",
        "plt.xlabel('äºˆæ¸¬ã•ã‚ŒãŸæŒ‡åç¢ºç‡')\n",
        "plt.ylabel('é »åº¦')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "training_predictions = final_model.predict_proba(X_train_fe)[:, 1]\n",
        "plt.hist(training_predictions, bins=50, alpha=0.7, color='lightcoral', edgecolor='black', label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿')\n",
        "plt.hist(final_submission['Drafted'], bins=50, alpha=0.7, color='skyblue', edgecolor='black', label='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿')\n",
        "plt.title('äºˆæ¸¬å€¤åˆ†å¸ƒã®æ¯”è¼ƒ')\n",
        "plt.xlabel('äºˆæ¸¬ã•ã‚ŒãŸæŒ‡åç¢ºç‡')\n",
        "plt.ylabel('é »åº¦')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\næœ€çµ‚æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: {final_submission.head()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–\n",
        "plt.figure(figsize=(12, 10))\\ntop_20_features = feature_importance_df.head(20)\\nsns.barplot(data=top_20_features, y='feature', x='importance', palette='viridis')\\nplt.title('ç‰¹å¾´é‡é‡è¦åº¦ TOP20ï¼ˆRandomForestï¼‰')\\nplt.xlabel('é‡è¦åº¦')\\nplt.ylabel('ç‰¹å¾´é‡')\\nplt.tight_layout()\\nplt.show()\\n\\n# æ–°è¦ä½œæˆç‰¹å¾´é‡ã®è©•ä¾¡\\nengineered_features = ['BMI', 'Power_Weight_Ratio', 'Speed_Power_Index', 'Agility_Score', \\n                      'Explosiveness', 'Speed_Explosiveness', 'Size_Index', 'Bench_Efficiency',\\n                      'OL_Strength_Index', 'Skill_Speed_Index', 'Defense_Athleticism',\\n                      'School_Draft_Rate', 'Total_Missing_Tests']\\n\\nengineered_importance = feature_importance_df[feature_importance_df['feature'].isin(engineered_features)]\\nprint(f\\\"\\\\n=== æ–°è¦ä½œæˆç‰¹å¾´é‡ã®é‡è¦åº¦ ===\\\")\\nprint(engineered_importance.sort_values('importance', ascending=False))\\n\\n# æ–°è¦ç‰¹å¾´é‡ã®ç·åˆè²¢çŒ®åº¦\\ntotal_engineered_importance = engineered_importance['importance'].sum()\\ntotal_importance = feature_importance_df['importance'].sum()\\nengineered_contribution = (total_engineered_importance / total_importance) * 100\\nprint(f\\\"\\\\næ–°è¦ç‰¹å¾´é‡ã®ç·åˆè²¢çŒ®åº¦: {engineered_contribution:.1f}%\\\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 10. æœ€çµ‚äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿äºˆæ¸¬\n",
        "def create_final_submission(X_train, y_train, X_test, test_ids, random_state=42):\\n    \\\"\\\"\\\"æœ€çµ‚çš„ãªæå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\\\"\\\"\\\"\\n    \\n    print(\\\"=== æœ€çµ‚ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­ ===\\\")\\n    \\n    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šï¼ˆå…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ï¼‰\\n    final_model = RandomForestClassifier(\\n        n_estimators=300,  # å°‘ã—å¢—ã‚„ã—ã¦ã¿ã‚‹\\n        max_depth=12,\\n        min_samples_split=8,\\n        min_samples_leaf=4,\\n        max_features='sqrt',\\n        class_weight='balanced',\\n        random_state=random_state,\\n        n_jobs=-1\\n    )\\n    \\n    # å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’\\n    final_model.fit(X_train, y_train)\\n    \\n    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬\\n    test_predictions = final_model.predict_proba(X_test)[:, 1]\\n    \\n    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\\n    submission = pd.DataFrame({\\n        'Id': test_ids,\\n        'Drafted': test_predictions\\n    })\\n    \\n    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\\n    submission_filename = 'improved_submission_with_feature_engineering.csv'\\n    submission.to_csv(submission_filename, index=False)\\n    \\n    print(f\\\"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {submission_filename}\\\")\\n    print(f\\\"äºˆæ¸¬å€¤ã®çµ±è¨ˆ:\\\")\\n    print(f\\\"  å¹³å‡: {test_predictions.mean():.4f}\\\")\\n    print(f\\\"  ä¸­å¤®å€¤: {np.median(test_predictions):.4f}\\\")\\n    print(f\\\"  æœ€å°å€¤: {test_predictions.min():.4f}\\\")\\n    print(f\\\"  æœ€å¤§å€¤: {test_predictions.max():.4f}\\\")\\n    print(f\\\"  æ¨™æº–åå·®: {test_predictions.std():.4f}\\\")\\n    \\n    return submission, final_model\\n\\n# æœ€çµ‚æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\\ntest_ids = test['Id'].values\\nfinal_submission, final_model = create_final_submission(X_train_fe, y_train_fe, X_test_fe, test_ids)\\n\\n# äºˆæ¸¬åˆ†å¸ƒã®å¯è¦–åŒ–\\nplt.figure(figsize=(12, 5))\\n\\nplt.subplot(1, 2, 1)\\nplt.hist(final_submission['Drafted'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\\nplt.title('ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿äºˆæ¸¬å€¤ã®åˆ†å¸ƒ')\\nplt.xlabel('äºˆæ¸¬ã•ã‚ŒãŸæŒ‡åç¢ºç‡')\\nplt.ylabel('é »åº¦')\\nplt.grid(True, alpha=0.3)\\n\\nplt.subplot(1, 2, 2)\\ntraining_predictions = final_model.predict_proba(X_train_fe)[:, 1]\\nplt.hist(training_predictions, bins=50, alpha=0.7, color='lightcoral', edgecolor='black', label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿')\\nplt.hist(final_submission['Drafted'], bins=50, alpha=0.7, color='skyblue', edgecolor='black', label='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿')\\nplt.title('äºˆæ¸¬å€¤åˆ†å¸ƒã®æ¯”è¼ƒ')\\nplt.xlabel('äºˆæ¸¬ã•ã‚ŒãŸæŒ‡åç¢ºç‡')\\nplt.ylabel('é »åº¦')\\nplt.legend()\\nplt.grid(True, alpha=0.3)\\n\\nplt.tight_layout()\\nplt.show()\\n\\nprint(f\\\"\\\\næœ€çµ‚æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: {final_submission.head()}\\\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 11. ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ”¹å–„çµæœã®ç·æ‹¬\\nprint(\\\"=== ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°çµæœã®ç·æ‹¬ ===\\\")\\nprint(f\\\"1. ç·ç‰¹å¾´é‡æ•°: {len(feature_cols_fe)}\\\")\\nprint(f\\\"2. äº¤å·®æ¤œè¨¼ã‚¹ã‚³ã‚¢: {np.mean(cv_scores_fe):.5f} Â± {np.std(cv_scores_fe):.5f}\\\")\\nprint(f\\\"3. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ: {np.mean(cv_scores_fe):.5f} vs 0.80792\\\")\\n\\nif np.mean(cv_scores_fe) > 0.80792:\\n    improvement = ((np.mean(cv_scores_fe) - 0.80792) / 0.80792) * 100\\n    print(f\\\"4. æ”¹å–„åº¦: +{improvement:.2f}%\\\")\\nelse:\\n    decline = ((0.80792 - np.mean(cv_scores_fe)) / 0.80792) * 100\\n    print(f\\\"4. æ”¹å–„åº¦: -{decline:.2f}%\\\")\\n\\nprint(f\\\"\\\\n=== ä½œæˆã—ãŸä¸»è¦ç‰¹å¾´é‡ ===\\\")\\nprint(\\\"- åŸºæœ¬ç‰¹å¾´é‡: BMI, Power_Weight_Ratio, Speed_Power_Index ãªã©\\\")\\nprint(\\\"- ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ç‰¹å¾´é‡: å„ãƒã‚¸ã‚·ãƒ§ãƒ³ç”¨ã®å°‚ç”¨æŒ‡æ¨™\\\")\\nprint(\\\"- å­¦æ ¡ãƒ—ãƒ¬ã‚¹ãƒ†ãƒ¼ã‚¸ç‰¹å¾´é‡: å­¦æ ¡ã®æŒ‡åå®Ÿç¸¾ã«åŸºã¥ãç‰¹å¾´é‡\\\")\\nprint(\\\"- æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡: æ¬ æã®æƒ…å ±ã‚‚ç‰¹å¾´é‡ã¨ã—ã¦æ´»ç”¨\\\")\\nprint(\\\"- ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ¥æ­£è¦åŒ–ç‰¹å¾´é‡: ãƒã‚¸ã‚·ãƒ§ãƒ³å†…ã§ã®ç›¸å¯¾çš„ãªèƒ½åŠ›\\\")\\n\\nprint(f\\\"\\\\n=== æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆã•ã‚‰ãªã‚‹æ”¹å–„æ¡ˆï¼‰ ===\\\")\\nprint(\\\"1. LightGBM/XGBoostãªã©ã®å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°æ‰‹æ³•ã®è©¦è¡Œ\\\")\\nprint(\\\"2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°æœ€é©åŒ–ï¼ˆOptunaç­‰ï¼‰\\\")\\nprint(\\\"3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ï¼‰\\\")\\nprint(\\\"4. ã‚ˆã‚Šé«˜åº¦ãªç‰¹å¾´é‡é¸æŠæ‰‹æ³•\\\")\\nprint(\\\"5. å¹´åº¦åˆ¥ã®æ™‚ç³»åˆ—åˆ†å‰²æ¤œè¨¼\\\")\\nprint(\\\"6. ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰\\\")\\n\\nprint(f\\\"\\\\n=== ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ› ===\\\")\\nprint(f\\\"- æ”¹è‰¯ç‰ˆæå‡ºãƒ•ã‚¡ã‚¤ãƒ«: improved_submission_with_feature_engineering.csv\\\")\\nprint(f\\\"- ç‰¹å¾´é‡é‡è¦åº¦ãŒä¿å­˜ã•ã‚Œã€åˆ†æã«æ´»ç”¨å¯èƒ½\\\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
