# NFL Draft Prediction - XGBoostãƒ¢ãƒ‡ãƒ«å®Ÿè£…

# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
import xgboost as xgb
import warnings

# è¨­å®š
warnings.filterwarnings('ignore')
np.random.seed(42)
pd.set_option('display.max_columns', None)

print("=== NFL Draft Prediction - XGBoostãƒ¢ãƒ‡ãƒ«å®Ÿè£…é–‹å§‹ ===")

# æ—¢å­˜ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåŒã˜ã‚‚ã®ï¼‰
def create_basic_features(df):
    """åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚’ä½œæˆ"""
    df_new = df.copy()
    
    # 1. BMIï¼ˆBody Mass Indexï¼‰
    df_new['BMI'] = df_new['Weight'] / (df_new['Height'] ** 2)
    
    # 2. ãƒ‘ãƒ¯ãƒ¼ãƒ»ã‚¦ã‚§ã‚¤ãƒˆãƒ»ãƒ¬ã‚·ã‚ª
    df_new['Power_Weight_Ratio'] = df_new['Bench_Press_Reps'] / df_new['Weight']
    
    # 3. ã‚¹ãƒ”ãƒ¼ãƒ‰ãƒ»ãƒ‘ãƒ¯ãƒ¼ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå‚ç›´è·³ã³ / 40ãƒ¤ãƒ¼ãƒ‰èµ°ï¼‰
    df_new['Speed_Power_Index'] = df_new['Vertical_Jump'] / df_new['Sprint_40yd']
    
    # 4. ã‚¢ã‚¸ãƒªãƒ†ã‚£ãƒ»ã‚¹ã‚³ã‚¢ï¼ˆ3cone + shuttleï¼‰
    df_new['Agility_Score'] = df_new['Agility_3cone'] + df_new['Shuttle']
    
    # 5. çˆ†ç™ºåŠ›æŒ‡æ¨™ï¼ˆç«‹ã¡å¹…è·³ã³ * å‚ç›´è·³ã³ï¼‰
    df_new['Explosiveness'] = df_new['Broad_Jump'] * df_new['Vertical_Jump']
    
    # 6. ã‚¹ãƒ”ãƒ¼ãƒ‰Ã—çˆ†ç™ºåŠ›
    df_new['Speed_Explosiveness'] = df_new['Explosiveness'] / df_new['Sprint_40yd']
    
    # 7. ä½“æ ¼æŒ‡æ¨™ï¼ˆèº«é•·Ã—ä½“é‡ï¼‰
    df_new['Size_Index'] = df_new['Height'] * df_new['Weight']
    
    # 8. ãƒ™ãƒ³ãƒãƒ—ãƒ¬ã‚¹åŠ¹ç‡ï¼ˆå›æ•°/ä½“é‡Ã—èº«é•·ï¼‰
    df_new['Bench_Efficiency'] = df_new['Bench_Press_Reps'] / df_new['Size_Index']
    
    # 9. å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆã‚«ãƒ†ã‚´ãƒªåŒ–ï¼‰
    df_new['Age_Group'] = pd.cut(df_new['Age'], bins=[0, 21, 23, 30], 
                                labels=['Young', 'Standard', 'Veteran'], 
                                include_lowest=True)
    
    print("åŸºæœ¬ç‰¹å¾´é‡ä½œæˆå®Œäº†")
    return df_new

def create_position_features(df):
    """ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ¥ã®ç‰¹å¾´é‡ã‚’ä½œæˆ"""
    df_new = df.copy()
    
    # ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã®æ­£è¦åŒ–ç‰¹å¾´é‡
    numerical_cols = ['Height', 'Weight', 'Sprint_40yd', 'Vertical_Jump', 
                     'Bench_Press_Reps', 'Broad_Jump', 'Agility_3cone', 'Shuttle']
    
    for col in numerical_cols:
        if col in df_new.columns:
            # ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã®å¹³å‡å€¤ã§æ­£è¦åŒ–
            position_means = df_new.groupby('Position_Type')[col].transform('mean')
            df_new[f'{col}_vs_Position_Mean'] = df_new[col] / position_means
            
            # ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã®æ¨™æº–åå·®ã§æ­£è¦åŒ–
            position_stds = df_new.groupby('Position_Type')[col].transform('std')
            df_new[f'{col}_Position_Zscore'] = (df_new[col] - position_means) / position_stds
    
    # ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ã®çµ„ã¿åˆã‚ã›ç‰¹å¾´é‡
    # 1. ã‚ªãƒ•ã‚§ãƒ³ã‚·ãƒ–ãƒ©ã‚¤ãƒ³ç”¨ç‰¹å¾´é‡
    offensive_line_mask = df_new['Position_Type'] == 'offensive_lineman'
    df_new['OL_Strength_Index'] = 0
    df_new.loc[offensive_line_mask, 'OL_Strength_Index'] = (
        df_new.loc[offensive_line_mask, 'Bench_Press_Reps'] * 
        df_new.loc[offensive_line_mask, 'Weight'] / 100
    )
    
    # 2. ã‚¹ã‚­ãƒ«ãƒã‚¸ã‚·ãƒ§ãƒ³ç”¨ç‰¹å¾´é‡
    skill_positions = ['backs_receivers']
    skill_mask = df_new['Position_Type'].isin(skill_positions)
    df_new['Skill_Speed_Index'] = 0
    df_new.loc[skill_mask, 'Skill_Speed_Index'] = (
        df_new.loc[skill_mask, 'Vertical_Jump'] * 
        df_new.loc[skill_mask, 'Broad_Jump'] / 
        df_new.loc[skill_mask, 'Sprint_40yd']
    )
    
    # 3. ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚¹ç”¨ç‰¹å¾´é‡
    defense_positions = ['defensive_lineman', 'line_backer', 'defensive_back']
    defense_mask = df_new['Position_Type'].isin(defense_positions)
    df_new['Defense_Athleticism'] = 0
    df_new.loc[defense_mask, 'Defense_Athleticism'] = (
        (df_new.loc[defense_mask, 'Vertical_Jump'] + 
         df_new.loc[defense_mask, 'Broad_Jump']) / 
        (df_new.loc[defense_mask, 'Sprint_40yd'] + 
         df_new.loc[defense_mask, 'Agility_3cone'])
    )
    
    print("ãƒã‚¸ã‚·ãƒ§ãƒ³ç‰¹åŒ–ç‰¹å¾´é‡ä½œæˆå®Œäº†")
    return df_new

def create_school_features(df):
    """å­¦æ ¡ã«é–¢ã™ã‚‹ç‰¹å¾´é‡ã‚’ä½œæˆ"""
    df_new = df.copy()
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ã£ã¦å­¦æ ¡çµ±è¨ˆã‚’è¨ˆç®—ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’é˜²ãï¼‰
    train_data = df_new[df_new['Drafted'].notna()].copy()
    
    # å­¦æ ¡åˆ¥ã®æŒ‡åç‡ã¨é¸æ‰‹æ•°
    school_stats = train_data.groupby('School').agg({
        'Drafted': ['count', 'mean']
    })
    school_stats.columns = ['School_Player_Count', 'School_Draft_Rate']
    
    # 5äººæœªæº€ã®å­¦æ ¡ã¯çµ±è¨ˆãŒä¸å®‰å®šãªã®ã§å¹³å‡å€¤ã§è£œå®Œ
    overall_draft_rate = train_data['Drafted'].mean()
    school_stats.loc[school_stats['School_Player_Count'] < 5, 'School_Draft_Rate'] = overall_draft_rate
    
    # å­¦æ ¡ãƒ—ãƒ¬ã‚¹ãƒ†ãƒ¼ã‚¸ã‚«ãƒ†ã‚´ãƒª
    def categorize_school_prestige(draft_rate, player_count):
        if pd.isna(draft_rate):
            return 'Unknown'
        elif draft_rate >= 0.8:
            return 'Elite'
        elif draft_rate >= 0.6:
            return 'High'
        elif draft_rate >= 0.4:
            return 'Medium'
        else:
            return 'Low'
    
    school_stats['School_Prestige'] = school_stats.apply(
        lambda x: categorize_school_prestige(x['School_Draft_Rate'], x['School_Player_Count']), axis=1
    )
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸
    df_new = df_new.merge(school_stats, left_on='School', right_index=True, how='left')
    
    # æ–°ã—ã„å­¦æ ¡ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã¿ã«å­˜åœ¨ï¼‰ã¯å¹³å‡å€¤ã§è£œå®Œ
    df_new['School_Draft_Rate'].fillna(overall_draft_rate, inplace=True)
    df_new['School_Player_Count'].fillna(1, inplace=True)
    df_new['School_Prestige'].fillna('Unknown', inplace=True)
    
    print("å­¦æ ¡ãƒ—ãƒ¬ã‚¹ãƒ†ãƒ¼ã‚¸ç‰¹å¾´é‡ä½œæˆå®Œäº†")
    return df_new

def advanced_missing_value_handling(df):
    """ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ¥ãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®æ¬ æå€¤å‡¦ç†"""
    df_new = df.copy()
    
    # èº«ä½“èƒ½åŠ›ãƒ†ã‚¹ãƒˆç³»ã®åˆ—
    performance_cols = ['Sprint_40yd', 'Vertical_Jump', 'Bench_Press_Reps', 
                       'Broad_Jump', 'Agility_3cone', 'Shuttle']
    
    # 1. æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ•ãƒ©ã‚°ã®ä½œæˆ
    for col in performance_cols:
        df_new[f'{col}_is_missing'] = df_new[col].isnull().astype(int)
    
    # 2. æ¬ æå€¤ã®ç·æ•°
    df_new['Total_Missing_Tests'] = df_new[[f'{col}_is_missing' for col in performance_cols]].sum(axis=1)
    
    # 3. ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ¥ã®ä¸­å¤®å€¤ã§è£œå®Œ
    for col in performance_cols:
        if col in df_new.columns:
            # ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã®ä¸­å¤®å€¤
            position_medians = df_new.groupby('Position_Type')[col].median()
            
            for position in position_medians.index:
                mask = (df_new['Position_Type'] == position) & (df_new[col].isnull())
                df_new.loc[mask, col] = position_medians[position]
            
            # æ®‹ã£ãŸæ¬ æå€¤ã¯å…¨ä½“ã®ä¸­å¤®å€¤ã§è£œå®Œ
            overall_median = df_new[col].median()
            df_new[col].fillna(overall_median, inplace=True)
    
    # 4. Age ã®è£œå®Œ
    if 'Age' in df_new.columns:
        # ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ¥ã®ä¸­å¤®å€¤ã§è£œå®Œ
        age_medians = df_new.groupby('Position_Type')['Age'].median()
        for position in age_medians.index:
            mask = (df_new['Position_Type'] == position) & (df_new['Age'].isnull())
            df_new.loc[mask, 'Age'] = age_medians[position]
        
        # æ®‹ã£ãŸæ¬ æå€¤ã¯å…¨ä½“ã®ä¸­å¤®å€¤ã§è£œå®Œ
        df_new['Age'].fillna(df_new['Age'].median(), inplace=True)
    
    print("æ”¹è‰¯ã•ã‚ŒãŸæ¬ æå€¤å‡¦ç†å®Œäº†")
    return df_new

def encode_categorical_features(df):
    """ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®åŠ¹æœçš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
    df_new = df.copy()
    
    # Label Encodingã™ã‚‹åˆ—
    label_encode_cols = ['School', 'Player_Type', 'Position_Type', 'Position', 
                        'Age_Group', 'School_Prestige']
    
    encoders = {}
    for col in label_encode_cols:
        if col in df_new.columns:
            le = LabelEncoder()
            df_new[col] = le.fit_transform(df_new[col].astype(str))
            encoders[col] = le
    
    # å¹´åº¦ã®å‡¦ç†ï¼ˆæ—¢ã«æ•°å€¤ãªã®ã§ãã®ã¾ã¾ï¼‰
    # å¹´åº¦ã®å‘¨æœŸæ€§ã‚’è€ƒæ…®ã—ãŸç‰¹å¾´é‡
    if 'Year' in df_new.columns:
        df_new['Year_sin'] = np.sin(2 * np.pi * (df_new['Year'] - df_new['Year'].min()) / 
                                   (df_new['Year'].max() - df_new['Year'].min()))
        df_new['Year_cos'] = np.cos(2 * np.pi * (df_new['Year'] - df_new['Year'].min()) / 
                                   (df_new['Year'].max() - df_new['Year'].min()))
    
    print("ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†")
    return df_new, encoders

def prepare_final_dataset(df, train_len):
    """æœ€çµ‚çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™"""
    # ä½¿ç”¨ã—ãªã„åˆ—ã‚’é™¤å¤–
    exclude_cols = ['Id', 'Drafted']
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    # æ•°å€¤å‹ä»¥å¤–ã®åˆ—ãŒã‚ã‚Œã°ç¢ºèª
    non_numeric_cols = df[feature_cols].select_dtypes(exclude=[np.number]).columns
    if len(non_numeric_cols) > 0:
        print(f"è­¦å‘Š: æ•°å€¤å‹ä»¥å¤–ã®åˆ—ãŒã‚ã‚Šã¾ã™: {list(non_numeric_cols)}")
        # å¼·åˆ¶çš„ã«æ•°å€¤å‹ã«å¤‰æ›
        for col in non_numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # ç„¡é™å¤§ã‚„æ¥µç«¯ã«å¤§ããªå€¤ã®å‡¦ç†
    for col in feature_cols:
        if col in df.columns:
            # ç„¡é™å¤§ã‚’ NaN ã«å¤‰æ›
            df[col] = df[col].replace([np.inf, -np.inf], np.nan)
            
            # æ®‹ã£ãŸ NaN ã‚’ä¸­å¤®å€¤ã§è£œå®Œ
            if df[col].isnull().sum() > 0:
                df[col].fillna(df[col].median(), inplace=True)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
    train_processed = df[:train_len].copy()
    test_processed = df[train_len:].copy()
    
    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢
    X_train = train_processed[feature_cols]
    y_train = train_processed['Drafted']
    X_test = test_processed[feature_cols]
    
    print(f"ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X_train.shape}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {X_test.shape}")
    print(f"æ¬ æå€¤ç¢ºèª - è¨“ç·´: {X_train.isnull().sum().sum()}, ãƒ†ã‚¹ãƒˆ: {X_test.isnull().sum().sum()}")
    
    return X_train, y_train, X_test, feature_cols

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
print("\n1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')
sample_submission = pd.read_csv('data/sample_submission_6_20ä¿®æ­£.csv')

print(f"Train: {train.shape}, Test: {test.shape}")

# ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒï¼‰
train_fe = train.copy()
test_fe = test.copy()

# å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãŸã‚ï¼‰
all_data = pd.concat([train_fe, test_fe], ignore_index=True)

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
all_data_fe = create_basic_features(all_data)
all_data_fe = create_position_features(all_data_fe)
all_data_fe = create_school_features(all_data_fe)
all_data_fe = advanced_missing_value_handling(all_data_fe)
all_data_fe, label_encoders = encode_categorical_features(all_data_fe)

# æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
X_train_fe, y_train_fe, X_test_fe, feature_cols_fe = prepare_final_dataset(all_data_fe, len(train))

# XGBoostãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ãƒ»è©•ä¾¡
def evaluate_xgboost_model(X_train, y_train, n_splits=5, random_state=42):
    """XGBoostãƒ¢ãƒ‡ãƒ«ã®äº¤å·®æ¤œè¨¼è©•ä¾¡"""
    
    # Stratified K-Foldè¨­å®š
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    
    # ã‚¹ã‚³ã‚¢æ ¼ç´ç”¨
    cv_scores = []
    feature_importance_sum = np.zeros(X_train.shape[1])
    
    print("\n2. XGBoost 5-Foldäº¤å·®æ¤œè¨¼å®Ÿè¡Œä¸­...")
    
    # XGBoostãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    xgb_params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'max_depth': 6,
        'learning_rate': 0.1,
        'n_estimators': 1000,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_weight': 1,
        'reg_alpha': 0.1,
        'reg_lambda': 1,
        'random_state': random_state,
        'verbosity': 0,
        'early_stopping_rounds': 100
    }
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
        
        # XGBoostãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»è¨“ç·´
        xgb_model = xgb.XGBClassifier(**xgb_params)
        
        # è¨“ç·´ï¼ˆæ—©æœŸåœæ­¢ä»˜ãï¼‰
        xgb_model.fit(
            X_fold_train, y_fold_train,
            eval_set=[(X_fold_train, y_fold_train), (X_fold_val, y_fold_val)],
            verbose=False
        )
        
        # äºˆæ¸¬ãƒ»è©•ä¾¡
        y_pred_proba = xgb_model.predict_proba(X_fold_val)[:, 1]
        fold_score = roc_auc_score(y_fold_val, y_pred_proba)
        cv_scores.append(fold_score)
        
        # ç‰¹å¾´é‡é‡è¦åº¦ã®ç´¯ç©
        feature_importance_sum += xgb_model.feature_importances_
        
        print(f"Fold {fold}: ROC AUC = {fold_score:.5f} (best_iteration: {xgb_model.best_iteration})")
    
    # çµæœã®é›†è¨ˆ
    mean_score = np.mean(cv_scores)
    std_score = np.std(cv_scores)
    feature_importance_avg = feature_importance_sum / n_splits
    
    print(f"\n=== XGBoost æœ€çµ‚çµæœ ===")
    print(f"å¹³å‡ROC AUC: {mean_score:.5f} Â± {std_score:.5f}")
    print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ: {mean_score:.5f} vs 0.80792")
    print(f"RandomForestã¨ã®æ¯”è¼ƒ: {mean_score:.5f} vs 0.83743")
    print(f"LightGBMã¨ã®æ¯”è¼ƒ: {mean_score:.5f} vs 0.83873")
    
    # æ”¹å–„çµæœã®è¡¨ç¤º
    best_previous = max(0.83743, 0.83873)  # RandomForestã¨LightGBMã®æœ€é«˜ã‚¹ã‚³ã‚¢
    if mean_score > best_previous:
        improvement = ((mean_score - best_previous) / best_previous) * 100
        print(f"ğŸ‰ æ–°è¨˜éŒ²! æœ€é«˜ã‚¹ã‚³ã‚¢æ›´æ–°: +{improvement:.2f}%")
    elif mean_score > 0.83743:
        improvement = ((mean_score - 0.83743) / 0.83743) * 100
        print(f"âœ… RandomForestæ”¹å–„: +{improvement:.2f}%")
    elif mean_score > 0.80792:
        improvement = ((mean_score - 0.80792) / 0.80792) * 100
        print(f"âœ… ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ”¹å–„: +{improvement:.2f}%")
    else:
        decline = ((0.80792 - mean_score) / 0.80792) * 100
        print(f"âš ï¸  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä¸‹å›ã‚Š: -{decline:.2f}%")
    
    return cv_scores, feature_importance_avg, xgb_params

# XGBoostãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡å®Ÿè¡Œ
cv_scores_xgb, feature_importance_xgb, best_params_xgb = evaluate_xgboost_model(X_train_fe, y_train_fe)

# ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æ
feature_importance_df_xgb = pd.DataFrame({
    'feature': feature_cols_fe,
    'importance': feature_importance_xgb
}).sort_values('importance', ascending=False)

print(f"\n=== XGBoost ç‰¹å¾´é‡é‡è¦åº¦TOP10 ===")
print(feature_importance_df_xgb.head(10))

# 3æ‰‹æ³•ã®ç‰¹å¾´é‡é‡è¦åº¦æ¯”è¼ƒ
print(f"\n=== 3æ‰‹æ³• ç‰¹å¾´é‡é‡è¦åº¦æ¯”è¼ƒ ===")
print("RandomForest TOP5:")
print("1. Age_Group (0.255767)")
print("2. Age (0.048578)")
print("3. Sprint_40yd_Position_Zscore (0.047013)")
print("4. School_Draft_Rate (0.044324)")
print("5. Sprint_40yd_vs_Position_Mean (0.039080)")

print(f"\nLightGBM TOP5:")
print("1. Age_Group (3088)")
print("2. School_Draft_Rate (426)")
print("3. Sprint_40yd_Position_Zscore (400)")
print("4. Weight_vs_Position_Mean (353)")
print("5. Sprint_40yd_vs_Position_Mean (345)")

print(f"\nXGBoost TOP5:")
for i, (_, row) in enumerate(feature_importance_df_xgb.head(5).iterrows(), 1):
    print(f"{i}. {row['feature']} ({row['importance']:.6f})")

# æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿äºˆæ¸¬
def create_xgboost_submission(X_train, y_train, X_test, test_ids, params, random_state=42):
    """XGBoostã§æœ€çµ‚çš„ãªæå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
    
    print("\n3. XGBoost æœ€çµ‚ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
    
    # å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§XGBoostãƒ¢ãƒ‡ãƒ«ä½œæˆ
    final_xgb_params = params.copy()
    final_xgb_params.pop('early_stopping_rounds', None)  # fitæ™‚ã«æŒ‡å®šã™ã‚‹ãŸã‚å‰Šé™¤
    
    final_xgb_model = xgb.XGBClassifier(**final_xgb_params)
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
    final_xgb_model.fit(X_train, y_train)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
    test_predictions = final_xgb_model.predict_proba(X_test)[:, 1]
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission = pd.DataFrame({
        'Id': test_ids,
        'Drafted': test_predictions
    })
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    submission_filename = 'xgboost_submission.csv'
    submission.to_csv(submission_filename, index=False)
    
    print(f"XGBoostæå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {submission_filename}")
    print(f"äºˆæ¸¬å€¤ã®çµ±è¨ˆ:")
    print(f"  å¹³å‡: {test_predictions.mean():.4f}")
    print(f"  ä¸­å¤®å€¤: {np.median(test_predictions):.4f}")
    print(f"  æœ€å°å€¤: {test_predictions.min():.4f}")
    print(f"  æœ€å¤§å€¤: {test_predictions.max():.4f}")
    print(f"  æ¨™æº–åå·®: {test_predictions.std():.4f}")
    
    return submission, final_xgb_model

# XGBoostæœ€çµ‚æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
test_ids = test['Id'].values
final_submission_xgb, final_xgb_model = create_xgboost_submission(X_train_fe, y_train_fe, X_test_fe, test_ids, best_params_xgb)

# 3æ‰‹æ³•ã®ç·åˆæ¯”è¼ƒ
print(f"\n=== ğŸ† 3æ‰‹æ³•ç·åˆæ¯”è¼ƒ ===")
print(f"RandomForest  : 0.83743 Â± 0.02740")
print(f"LightGBM      : 0.83873 Â± 0.01730")
print(f"XGBoost       : {np.mean(cv_scores_xgb):.5f} Â± {np.std(cv_scores_xgb):.5f}")
print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³  : 0.80792")

# æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’ç‰¹å®š
scores = {
    'RandomForest': 0.83743,
    'LightGBM': 0.83873,
    'XGBoost': np.mean(cv_scores_xgb)
}

best_model = max(scores, key=scores.get)
best_score = scores[best_model]

print(f"\nğŸ† ç¾åœ¨ã®æœ€é«˜ã‚¹ã‚³ã‚¢: {best_model} ({best_score:.5f})")
print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ”¹å–„: +{((best_score - 0.80792) / 0.80792) * 100:.2f}%")

print(f"\n=== XGBoostå®Ÿè£…å®Œäº† ===")
print(f"- æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: xgboost_submission.csv")
print(f"- æ¬¡ã®æ¨å¥¨ä½œæ¥­: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ï¼ˆ3æ‰‹æ³•çµ±åˆï¼‰") 